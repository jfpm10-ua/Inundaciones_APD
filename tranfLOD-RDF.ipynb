{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56324728",
   "metadata": {},
   "source": [
    "### Preparacion\n",
    "\n",
    "Importar librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81e4d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdflib import Graph, URIRef, Literal, Namespace\n",
    "from rdflib.namespace import RDF, XSD, OWL\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe73447a",
   "metadata": {},
   "source": [
    "### Espacios de nombres & Transformacion a RDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37cfbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_en_wikidata(nombre_ciudad):\n",
    "    url = \"https://query.wikidata.org/sparql\"\n",
    "    \n",
    "    # La consulta SPARQL simple\n",
    "    query = f\"\"\"\n",
    "    SELECT ?item WHERE {{\n",
    "      ?item rdfs:label \"{nombre_ciudad}\"@en.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }} LIMIT 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parámetros de la petición GET\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'format': 'json'\n",
    "    }\n",
    "    \n",
    "    # Wikidata exige User-Agent\n",
    "    headers = {\n",
    "        'User-Agent': 'PrediccionInundaciones/1.0 (direccion@ejemplo.com)'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Hacemos la petición GET (igual que escribir la URL en el navegador)\n",
    "        r = requests.get(url, params=params, headers=headers)\n",
    "        data = r.json()\n",
    "        \n",
    "        # Navegamos por el JSON de respuesta para sacar el ID (Qxxxx)\n",
    "        if data['results']['bindings']:\n",
    "            return data['results']['bindings'][0]['item']['value']\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error de conexión: {e}\")\n",
    "        return None\n",
    "\n",
    "# Cache para guardar resultados y no repetir peticiones\n",
    "cache_ciudades = {}\n",
    "\n",
    "# --- 2. PREPARACIÓN DEL GRAFO ---\n",
    "domain = 'https://prediccion-inundaciones.es/'\n",
    "schema = Namespace(\"https://schema.org/\")\n",
    "\n",
    "g = Graph()\n",
    "g.bind(\"rdf\", RDF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"owl\", OWL)\n",
    "g.bind(\"schema\", schema)\n",
    "\n",
    "df = pd.read_csv(\"datos_combinados.csv\", sep=';')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Procesando datos...\")\n",
    "\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    ciudad = str(row['Ciudad']).strip() \n",
    "    fecha = str(row['fecha']).strip()\n",
    "    hora = str(row['hora']).strip()\n",
    "    \n",
    "    # place\n",
    "    city_slug = ciudad.lower().replace(\" \", \"_\")\n",
    "    place_uri = URIRef(domain + 'location/' + city_slug)\n",
    "    \n",
    "    # Consultamos Wikidata si es una ciudad nueva para nosotros\n",
    "    if ciudad not in cache_ciudades:\n",
    "        print(f\"Buscando '{ciudad}' en Wikidata...\")\n",
    "        uri_wikidata = buscar_en_wikidata(ciudad)\n",
    "        cache_ciudades[ciudad] = uri_wikidata\n",
    "        time.sleep(0.5) # pausa entre peticiones\n",
    "    \n",
    "    g.add((place_uri, RDF.type, schema.Place))\n",
    "    g.add((place_uri, schema.name, Literal(ciudad)))\n",
    "    \n",
    "    # Si encontramos enlace en Wikidata, lo añadimos (Enriquecimiento)\n",
    "    if cache_ciudades[ciudad]:\n",
    "        g.add((place_uri, OWL.sameAs, URIRef(cache_ciudades[ciudad])))\n",
    "\n",
    "    # event\n",
    "    timestamp = f\"{fecha}T{hora}:00\"\n",
    "    \n",
    "    # ID único\n",
    "    safe_time = timestamp.replace(\"-\",\"\").replace(\":\",\"\").replace(\"T\",\"_\")\n",
    "    event_id = f\"evento_{city_slug}_{safe_time}\"\n",
    "    event_uri = URIRef(domain + 'event/' + event_id)\n",
    "    \n",
    "    g.add((event_uri, RDF.type, schema.Event))\n",
    "    g.add((event_uri, schema.name, Literal(f\"Medición en {ciudad}\")))\n",
    "    g.add((event_uri, schema.startDate, Literal(timestamp,\n",
    "                                             datatype=XSD.dateTime)))\n",
    "    \n",
    "    t = row.get('Temperature', 'N/A')\n",
    "    p = row.get('precipitacion_media', 'N/A')\n",
    "    c = row.get('Discharge', 'N/A')\n",
    "    h = row.get('Humidity', 'N/A')\n",
    "    \n",
    "    desc = f\"Temp: {t}C, Lluvia: {p}mm, Caudal: {c} m3/s, Humedad: {h}%\"\n",
    "    g.add((event_uri, schema.description, Literal(desc)))\n",
    "    \n",
    "    # relacion\n",
    "    g.add((place_uri, schema.event, event_uri))\n",
    "    g.add((event_uri, schema.location, place_uri))\n",
    "\n",
    "# guardar\n",
    "g.serialize(destination=\"hidrologia_final.ttl\", format=\"turtle\")\n",
    "print(\"¡Proceso completado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
